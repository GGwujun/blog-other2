<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-other2/umi.3ec1f225.css" />
    <script>
      window.routerBase = "/blog-other2";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>34 | 连续序列化模型：线性动态系统 - 大师兄</title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/机器学习40讲/04.概率图模型/07" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-other2/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>大数据<ul><li><a href="/blog-other2/从0开始学大数据">从0开始学大数据</a></li></ul></span><span>架构师<ul><li><a href="/blog-other2/sre实战手册">sre实战手册</a></li><li><a href="/blog-other2/vim实用技巧必知必会">vim实用技巧必知必会</a></li><li><a href="/blog-other2/深入浅出云计算">深入浅出云计算</a></li><li><a href="/blog-other2/说透数字化转型">说透数字化转型</a></li><li><a href="/blog-other2/遗留系统现代化实战">遗留系统现代化实战</a></li></ul></span><span>工作生活<ul><li><a aria-current="page" class="active" href="/blog-other2/机器学习40讲">机器学习40讲</a></li><li><a href="/blog-other2/职场求生攻略">职场求生攻略</a></li></ul></span><span>杂谈<ul><li><a href="/blog-other2/aI技术内参">aI技术内参</a></li><li><a href="/blog-other2/人工智能基础课">人工智能基础课</a></li><li><a href="/blog-other2/反爬虫兵法演绎20讲">反爬虫兵法演绎20讲</a></li><li><a href="/blog-other2/实用密码学">实用密码学</a></li><li><a href="/blog-other2/手把手带你写一门编程语言">手把手带你写一门编程语言</a></li><li><a href="/blog-other2/深入浅出区块链">深入浅出区块链</a></li><li><a href="/blog-other2/说透区块链">说透区块链</a></li><li><a href="/blog-other2/超级访谈对话汤峥嵘">超级访谈对话汤峥嵘</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-other2/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>大数据<ul><li><a href="/blog-other2/从0开始学大数据">从0开始学大数据</a></li></ul></li><li>架构师<ul><li><a href="/blog-other2/sre实战手册">sre实战手册</a></li><li><a href="/blog-other2/vim实用技巧必知必会">vim实用技巧必知必会</a></li><li><a href="/blog-other2/深入浅出云计算">深入浅出云计算</a></li><li><a href="/blog-other2/说透数字化转型">说透数字化转型</a></li><li><a href="/blog-other2/遗留系统现代化实战">遗留系统现代化实战</a></li></ul></li><li>工作生活<ul><li><a aria-current="page" class="active" href="/blog-other2/机器学习40讲">机器学习40讲</a></li><li><a href="/blog-other2/职场求生攻略">职场求生攻略</a></li></ul></li><li>杂谈<ul><li><a href="/blog-other2/aI技术内参">aI技术内参</a></li><li><a href="/blog-other2/人工智能基础课">人工智能基础课</a></li><li><a href="/blog-other2/反爬虫兵法演绎20讲">反爬虫兵法演绎20讲</a></li><li><a href="/blog-other2/实用密码学">实用密码学</a></li><li><a href="/blog-other2/手把手带你写一门编程语言">手把手带你写一门编程语言</a></li><li><a href="/blog-other2/深入浅出区块链">深入浅出区块链</a></li><li><a href="/blog-other2/说透区块链">说透区块链</a></li><li><a href="/blog-other2/超级访谈对话汤峥嵘">超级访谈对话汤峥嵘</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-other2/机器学习40讲">机器学习40讲</a></li><li><a href="/blog-other2/机器学习40讲/01.开篇词">01.开篇词</a><ul><li><a href="/blog-other2/机器学习40讲/01.开篇词/01"><span>开篇词 | 打通修炼机器学习的任督二脉</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观">02.机器学习概观</a><ul><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/01"><span>01 | 频率视角下的机器学习</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/02"><span>02 | 贝叶斯视角下的机器学习</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/03"><span>03 | 学什么与怎么学</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/04"><span>04 | 计算学习理论</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/05"><span>05 | 模型的分类方式</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/06"><span>06 | 模型的设计准则</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/07"><span>07 | 模型的验证方法</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/08"><span>08 | 模型的评估指标</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/09"><span>09 | 实验设计</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/10"><span>10 | 特征预处理</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型">03.统计机器学习模型</a><ul><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/01"><span>11 | 基础线性回归：一元与多元</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/02"><span>12 | 正则化处理：收缩方法与边际化</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/03"><span>13 | 线性降维：主成分的使用</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/04"><span>14 | 非线性降维：流形学习</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/05"><span>15 | 从回归到分类：联系函数与降维</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/06"><span>16 | 建模非正态分布：广义线性模型</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/07"><span>17 | 几何角度看分类：支持向量机</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/08"><span>18 | 从全局到局部：核技巧</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/09"><span>19 | 非参数化的局部模型：K近邻</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/10"><span>20 | 基于距离的学习：聚类与度量学习</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/11"><span>21 | 基函数扩展：属性的非线性化</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/12"><span>22 | 自适应的基函数：神经网络</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/13"><span>23 | 层次化的神经网络：深度学习</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/14"><span>24 | 深度编解码：表示学习</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/15"><span>25 | 基于特征的区域划分：树模型</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/16"><span>26 | 集成化处理：Boosting与Bagging</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/17"><span>27 | 万能模型：梯度提升与随机森林</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/18"><span>总结课 | 机器学习的模型体系</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-other2/机器学习40讲/04.概率图模型">04.概率图模型</a><ul><li><a href="/blog-other2/机器学习40讲/04.概率图模型/01"><span>28 | 最简单的概率图：朴素贝叶斯</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/02"><span>29 | 有向图模型：贝叶斯网络</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/03"><span>30 | 无向图模型：马尔可夫随机场</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/04"><span>31 | 建模连续分布：高斯网络</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/05"><span>32 | 从有限到无限：高斯过程</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/06"><span>33 | 序列化建模：隐马尔可夫模型</span></a></li><li><a aria-current="page" class="active" href="/blog-other2/机器学习40讲/04.概率图模型/07"><span>34 | 连续序列化模型：线性动态系统</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/08"><span>35 | 精确推断：变量消除及其拓展</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/09"><span>36 | 确定近似推断：变分贝叶斯</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/10"><span>37 | 随机近似推断：MCMC</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/11"><span>38 | 完备数据下的参数学习：有向图与无向图</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/12"><span>39 | 隐变量下的参数学习：EM方法与混合模型</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/13"><span>40 | 结构学习：基于约束与基于评分</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/14"><span>总结课 | 贝叶斯学习的模型体系</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/05.结束语">05.结束语</a><ul><li><a href="/blog-other2/机器学习40讲/05.结束语/01"><span>结课 | 终有一天，你将为今天的付出骄傲</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/06.加餐">06.加餐</a><ul><li><a href="/blog-other2/机器学习40讲/06.加餐/01"><span>如何成为机器学习工程师？</span></a></li><li><a href="/blog-other2/机器学习40讲/06.加餐/02"><span>结课测试 | 这些机器学习知识你都掌握了吗？</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/summary">机器学习40讲</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="34--连续序列化模型线性动态系统"><a aria-hidden="true" tabindex="-1" href="/blog-other2/机器学习40讲/04.概率图模型/07#34--连续序列化模型线性动态系统"><span class="icon icon-link"></span></a>34 | 连续序列化模型：线性动态系统</h1><p>在隐马尔可夫模型中，一般的假设是状态和观测都是离散的随机变量。如果假定隐藏的状态变量满足连续分布，那么得到的就是线性动态系统。虽然这个概念更多地出现在信号处理与控制论中，看起来和机器学习风马牛不相及，但是从马尔可夫性和贝叶斯概率的角度观察，<strong>线性系统也是一类重要的处理序列化数据的模型</strong>。</p><p><strong>线性动态系统</strong>（linear dynamical system）的作用可以通过下面这个例子来说明。假设一个传感器被用于测量未知的物理量$z$，但测量结果$x$会受到零均值高斯噪声的影响。在单次测量中，根据最大似然可以得到，对未知的$z$最优的估计值就是测量结果本身，也就是令$z = x$。可是如果可以对$z$进行多次重复测量的话，就可以通过求解这些结果的平均来平滑掉随机噪声的影响，从而计算出更加精确的估计。</p><p>可一旦多次测量结果是在不同的时间点上测出的，也就是时间序列$x_1, x_2, \cdots, x_N$时，问题就没有那么简单了，因为这种情况下需要将未知变量$z$的<strong>时变特性</strong>考虑进去，前一时刻的$z$和后一时刻的$z$就不一样了。如果还是像上面那样直接对不同时刻的测量结果求均值的话，虽然随机噪声的影响可以被平滑掉，但变量本身的时变特性又会作为另一种噪声出现在结果中。</p><p>这种按倒葫芦起了瓢的结果显然不是我们想要的。要想获得更精确的估计，可以根据$z$的具体变化特性采用不同的策略。如果变量$z$具有慢变的特性，也就是它的波动周期远远大于测量周期，那么不同时刻的测量结果中的差别几乎全部来源于随机噪声，变量本身并不会出现多少波动。这时就可以将平滑窗口的长度，也就是用于求平均的测量结果的数目取得大一些，以取得更好的抑制随机噪声的效果。</p><p>反过来，如果变量本身是快速变化的，这时就要适当地调小平滑窗口，同时给相距较近的测量结果赋予更大的平滑权重。如果变量变化的时间尺度比相邻测量之间的时间间隔还要小，两次测量之间都能变量都能经历多次波动，就只能以测量结果作为最优的估计了。</p><p>上面的描述只是定性的说明，如果要在此基础上对最优的平滑参数进行定量计算的话，就需要对系统的时间演化和测量过程进行概率式的建模，其模型就是线性动态系统。</p><p><strong>线性动态系统也是一种动态贝叶斯网络，其中的显式变量和隐藏变量都是连续变量，它们之间的依赖关系则是线性高斯的条件分布</strong>。最典型的线性动态系统是用于描述一个或一组高斯噪声影响下的实值变量随时间变化的过程，可以用以下的条件概率表示：</p><p>$$ P(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- --> | <!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n - 1)<!-- -->}<!-- -->) = \mathscr<!-- -->{<!-- -->N<!-- -->}<!-- --> (<!-- -->{<!-- -->\bf A<!-- -->}<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n - 1)<!-- -->}<!-- -->, <!-- -->{<!-- -->\bf Q<!-- -->}<!-- -->), P(<!-- -->{<!-- -->\bf O<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- --> | <!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->) = \mathscr<!-- -->{<!-- -->N<!-- -->}<!-- --> (<!-- -->{<!-- -->\bf H<!-- -->}<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->, <!-- -->{<!-- -->\bf R<!-- -->}<!-- -->) $$</p><p>其中$\bf X$是$n$维的隐藏状态变量，$\bf O$是$m$维的观测变量，$\bf A$是定义了模型的线性转化规则的$n$维方阵，$\bf Q$是定义了状态随时间演化过程中的高斯噪声的$n$维方阵，$\bf H$是定义了从状态到观测的线性转化规则的$n \times m$维矩阵，$\bf R$是定义了观测结果中高斯噪声的$m$维方阵。隐藏状态变量初始的取值$<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(0)<!-- -->}<!-- -->$也满足高斯分布，其概率密度可以写成</p><p>$$ P(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(0)<!-- -->}<!-- -->) = \mathscr<!-- -->{<!-- -->N<!-- -->}<!-- --> (<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(0)<!-- -->}<!-- -->| \boldsymbol \mu_0, V_0) $$</p><p>如果将线性动态系统放在状态空间表象（state space representation）下观察，上面的条件概率就可以改写成状态方程的形式</p><p>$$ <!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- --> = <!-- -->{<!-- -->\bf A<!-- -->}<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n - 1)<!-- -->}<!-- --> + <!-- -->{<!-- -->\bf w<!-- -->}<!-- --> $$</p><p>$$ <!-- -->{<!-- -->\bf O<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- --> = <!-- -->{<!-- -->\bf H<!-- -->}<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- --> + <!-- -->{<!-- -->\bf v<!-- -->}<!-- --> $$</p><p>$$ <!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(0)<!-- -->}<!-- --> = <!-- -->{<!-- -->\bf t<!-- -->}<!-- -->^<!-- -->{<!-- -->(0)<!-- -->}<!-- --> + <!-- -->{<!-- -->\bf u<!-- -->}<!-- --> $$</p><p>其中$<!-- -->{<!-- -->\bf w<!-- -->}<!-- -->, <!-- -->{<!-- -->\bf v<!-- -->}<!-- -->, <!-- -->{<!-- -->\bf u<!-- -->}<!-- -->$都是均值为0的高斯分布。</p><p>对原始的线性动态系统模型的一种扩展方式是将系统中的初始状态$<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(0)<!-- -->}<!-- -->$从单个的高斯分布改写为高斯混合模型。如果高斯混合模型中包含$K$个成分，那么后面所有状态也会是多个高斯分布的混合，通过求解边界概率依然可以对模型进行概率推断。</p><p>除了状态本身的概率分布外，从状态到观测的观测概率也可以写成高斯混合模型的形式。如果观测概率是$K$个高斯分布的混合，那么初始状态$<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(0)<!-- -->}<!-- -->$的后验概率同样是$K$个高斯分布的混合。随着时序的推进，后验概率的表示会变的越来越复杂。在时刻$n$，状态$<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->$的后验概率数目可以达到$K ^ n$个，这时参数的指数式增长就会削弱模型的实用性。</p><p>从隐马尔可夫模型和线性系统出发，可以定义出一些非传统机器学习意义上的概念。如果要根据截至目前的观测结果来估计隐藏变量，这样的问题就是滤波问题（filtering）；如果要根据一串完整的观测序列来估计隐藏变量，这样的问题就是平滑问题（smoothing）。滤波和平滑对隐变量的估计都属于概率图模型推断任务的范畴，在离散的隐马尔可夫模型和连续的线性动态系统中都可以实现。</p><p>另一个只适用于离散模型的运算是译码（decoding），其任务是根据观测序列找到后验概率最大的隐变量序列。这些术语被更广泛地应用在通信和信息处理之中，但是站在更宏观的角度看，它们也可以归结到广义的机器学习范畴之中。</p><p>如果要在离散的时间序列上分析连续分布的状态和观测结果的话，需要使用线性卡尔曼滤波器这个数学工具。<strong>线性卡尔曼滤波器</strong>（linear Kalman filter）的作用是根据一串包含统计噪声和干扰的观测结果来计算出单一的、但是更加精确的观测估计。在某个时间点上，给定截止目前所获得的所有证据，可以计算出关于系统的当前状态的置信状态（belief state），其中包含了最大的信息量。在离散的时序上，时刻$n$的置信状态可以定义为</p><p>$$ \sigma^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->) = P(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- --> | o^<!-- -->{<!-- -->(0: n)<!-- -->}<!-- -->) $$</p><p>其中$<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->$表示当前的状态，$o^<!-- -->{<!-- -->(0: n-1)<!-- -->}<!-- -->$表示之前所有的观测。在线性高斯的依赖关系下，置信状态服从高斯分布，因而可以用参数有限的均值向量和协方差矩阵来表示，具有紧凑的形式。<strong>卡尔曼滤波器的作用就是对置信状态的均值和协方差进行更新</strong>，更新的过程可以分成两个步骤：首先根据目前所有可用的观测计算出隐藏状态变量的置信，这个置信叫作<strong>先验置信状态</strong>（prior belief state），其表达式可以写成</p><p>$$ \sigma^<!-- -->{<!-- -->(\cdot n+1)<!-- -->}<!-- -->(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- -->) = P(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> | o^<!-- -->{<!-- -->(0: n)<!-- -->}<!-- -->) = \sum\limits_<!-- -->{<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->}<!-- --> P(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> | <!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->) \sigma^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n)<!-- -->}<!-- -->) $$</p><p>式子中上标的$\cdot$是先验置信状态的标志。接下来，在这个先验置信状态的条件下要考虑最近的观测结果，根据推测出的先验置信状态和当前的观测结果共同确定当前的置信状态，其表达式可以写成</p><p>$$ \sigma^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- -->(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- -->) = P(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> | o^<!-- -->{<!-- -->(0: n)<!-- -->}<!-- -->, o^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- -->) = \dfrac<!-- -->{<!-- -->P(o^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> | <!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- -->)\sigma^<!-- -->{<!-- -->(\cdot n+1)<!-- -->}<!-- -->(<!-- -->{<!-- -->\bf X<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- -->)<!-- -->}<!-- -->{<!-- -->P(o^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> | o^<!-- -->{<!-- -->(0: n)<!-- -->}<!-- -->)<!-- -->}<!-- --> $$</p><p>这个递归过滤的过程对随时间变化的置信状态进行动态更新，可以看成是递归贝叶斯估计在多元高斯分布中的应用。<strong>递归贝叶斯估计</strong>（recursive Bayesian estimation）通过对不同时间观测值的递归使用来估计未知的概率分布，<strong>是隐马尔可夫模型中常用的推理方法。线性卡尔曼滤波器就是递归贝叶斯估计在连续分布的状态变量上的推广</strong>。</p><p>在卡尔曼滤波过程中，先验置信状态的更新由状态转移更新（state transition update）来表示。以上面的线性系统为例，直观上看，先验置信状态的新均值$\mu^<!-- -->{<!-- -->\cdot t+1<!-- -->}<!-- -->$是将状态转移的线性变换$\bf A$应用在上个时间点的均值$\mu^<!-- -->{<!-- -->\cdot t<!-- -->}<!-- -->$上，新的协方差矩阵$\Sigma^<!-- -->{<!-- -->\cdot t+1<!-- -->}<!-- -->$也是将状态转移的线性变换$\bf A$应用在上个时间点的协方差$\Sigma^<!-- -->{<!-- -->\cdot t<!-- -->}<!-- -->$上，再加上噪声的协方差$\bf Q$。总而言之，一个变换就可以将状态更新全部概括。</p><p>和状态转移更新相比，确定当前置信状态的<strong>观测更新</strong>（observation update）要复杂一些。置信状态的均值可以表示为先验置信的均值加上来源于观测结果的修正项，这个修正项是观测残差（observation residual），也就是期望观测值和实际观测值之间区别的加权，加权系数是被称为卡尔曼增益（Kalman gain）的系数矩阵$\bf K$。</p><p>置信状态的协方差也是对先验置信的协方差进行修正，修正的方式是减去卡尔曼增益加权后的期望协方差。在数学推导中，置信状态的参数表达式可以写成</p><p>$$ <!-- -->{<!-- -->\bf K<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> = \Sigma^<!-- -->{<!-- -->(\cdot n+1)<!-- -->}<!-- -->{<!-- -->\bf H<!-- -->}<!-- -->^T (<!-- -->{<!-- -->\bf H<!-- -->}<!-- --> \Sigma^<!-- -->{<!-- -->(\cdot n+1)<!-- -->}<!-- -->{<!-- -->\bf H<!-- -->}<!-- -->^T + <!-- -->{<!-- -->\bf R<!-- -->}<!-- -->)^<!-- -->{<!-- -->-1<!-- -->}<!-- --> $$</p><p>$$ \mu^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> = \mu^<!-- -->{<!-- -->(\cdot n+1)<!-- -->}<!-- --> + <!-- -->{<!-- -->\bf K<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- -->(<!-- -->{<!-- -->\bf o<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> - <!-- -->{<!-- -->\bf H<!-- -->}<!-- --> \Sigma^<!-- -->{<!-- -->(\cdot n+1)<!-- -->}<!-- -->) $$</p><p>$$ \Sigma^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- --> = (<!-- -->{<!-- -->\bf I<!-- -->}<!-- --> - <!-- -->{<!-- -->\bf K<!-- -->}<!-- -->^<!-- -->{<!-- -->(n+1)<!-- -->}<!-- -->{<!-- -->\bf H<!-- -->}<!-- -->)\Sigma^<!-- -->{<!-- -->(\cdot n+1)<!-- -->}<!-- --> $$</p><p><strong>卡尔曼增益是卡尔曼滤波器中的核心参数，它体现的是观测的重要性</strong>。当表示测量误差的协方差矩阵$\bf R$趋近于0时，卡尔曼增益会收敛到状态-观测矩阵的逆$<!-- -->{<!-- -->\bf H<!-- -->}<!-- -->^<!-- -->{<!-- -->-1<!-- -->}<!-- -->$，这意味着真实的观测结果在置信状态的更新中扮演着越发重要的角色，预测的观测结果的地位则会不断下降。在此基础上进一步可以得到，置信状态在更新后，其协方差趋近于0，表示当前的状态几乎是确定的。</p><p>反过来，当先验置信的协方差矩阵$\Sigma^<!-- -->{<!-- -->(\cdot n+1)<!-- -->}<!-- -->$趋近于0时，计算出的卡尔曼增益也会趋近于0。这种情况下，在置信更新中占据主导地位的就变成了对分布的预测结果，后验置信和先验置信的统计特性几乎完全一致，观测结果反而变得无足轻重。随着时序的推移，置信状态最终会收敛到某个分布上，系统的不确定性也会呈现出稳定的状态。</p><p>上面介绍的是最原始的卡尔曼滤波器，其原理可以推广到更加复杂的情况中。如果将状态转移和状态到观测的关系建模为非线性关系，并用泰勒展开中的一阶导数和二阶导数进行局部的线性化处理，这样的卡尔曼滤波器就是<strong>扩展卡尔曼滤波器</strong>（extended Kalman filter）。</p><p>另一种用于非线性动态的改进是<strong>无迹卡尔曼滤波器</strong>（unscented Kalman filter），它是通过无迹变换来对非线性动态进行线性化。两者的具体细节在此就不介绍了，感兴趣的话你可以自行查阅资料了解。</p><p>无论是扩展卡尔曼还是无迹卡尔曼，都是对非线性特性的确定性近似（deterministic approximation）。它们放弃了精确计算的追求，转而以确定的方式求解近似的结果。这里的确定性指的是只要用相同的方式进行近似，每次得到的结果都是一样的。如果用随机的方式来对非线性特性做出近似，对应的方法就是<strong>粒子滤波</strong>（particle filter）。</p><p>粒子滤波的任务也是根据观测结果估计隐藏的状态。它并不通过复杂的积分计算出准确的结果，而是对总体的分布进行采样，用样本的经验分布来代替总体的真实分布，用样本的均值来代替总体的积分运算。</p><p>由于这种方法和蒙特卡洛采样的思路近似，因而也被称为序贯蒙特卡洛（sequential Monte Carlo）。作为一种非参数方法，粒子滤波可以用来建模任意形式的概率分布，虽然效果未必有多好，很多情况下却是唯一可行的方法。</p><p>粒子滤波用样本的平均值来代替总体分布的数学期望，但这里的平均值不是一般的平均，每个样本点都被赋予一个权值，样本及其权重共同构成了粒子滤波中的“粒子”。整个粒子滤波的过程就是动态调整样本的权重，使经验分布不断接近真实分布的过程，具体的数学细节在这里就不讨论了。</p><p>今天我和你分享了线性动态系统和一些滤波算法的基本原理，包含以下四个要点：</p><ul><li><p>线性动态系统是具有连续状态变量的隐马尔可夫模型，所有条件概率都是线性高斯分布；</p></li><li><p>线性动态系统的求解是根据先验置信状态和观测结果来更新系统的置信状态；</p></li><li><p>卡尔曼滤波器可以对线性动态系统进行精确求解；</p></li><li><p>当系统具有非线性和非高斯特性时，可以通过扩展卡尔曼滤波器、无迹卡尔曼滤波器和粒子滤波等方法求解。</p></li></ul><p>卡尔曼滤波器及其变种在动态的运动目标跟踪中有广泛的应用，是机器人感知、定位与导航的一种重要方法。</p><p>你可以查阅资料，了解卡尔曼滤波器具体的应用方式，并在这里分享你的见解。</p><p><img src="/blog-other2/static/httpsstatic001geekbangorgresourceimage934093feb627e6f15344f030eb48a635b240.30f802e4.jpg" alt=""/></p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/机器学习40讲/04.概率图模型/07.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/29 16:57:20</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-other2/umi.2006439a.js"></script>
  </body>
</html>
