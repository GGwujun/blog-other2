<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-other2/umi.3ec1f225.css" />
    <script>
      window.routerBase = "/blog-other2";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>19 | 非参数化的局部模型：K近邻 - 大师兄</title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/机器学习40讲/03.统计机器学习模型/09" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-other2/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>大数据<ul><li><a href="/blog-other2/从0开始学大数据">从0开始学大数据</a></li></ul></span><span>架构师<ul><li><a href="/blog-other2/sre实战手册">sre实战手册</a></li><li><a href="/blog-other2/vim实用技巧必知必会">vim实用技巧必知必会</a></li><li><a href="/blog-other2/深入浅出云计算">深入浅出云计算</a></li><li><a href="/blog-other2/说透数字化转型">说透数字化转型</a></li><li><a href="/blog-other2/遗留系统现代化实战">遗留系统现代化实战</a></li></ul></span><span>工作生活<ul><li><a aria-current="page" class="active" href="/blog-other2/机器学习40讲">机器学习40讲</a></li><li><a href="/blog-other2/职场求生攻略">职场求生攻略</a></li></ul></span><span>杂谈<ul><li><a href="/blog-other2/aI技术内参">aI技术内参</a></li><li><a href="/blog-other2/人工智能基础课">人工智能基础课</a></li><li><a href="/blog-other2/反爬虫兵法演绎20讲">反爬虫兵法演绎20讲</a></li><li><a href="/blog-other2/实用密码学">实用密码学</a></li><li><a href="/blog-other2/手把手带你写一门编程语言">手把手带你写一门编程语言</a></li><li><a href="/blog-other2/深入浅出区块链">深入浅出区块链</a></li><li><a href="/blog-other2/说透区块链">说透区块链</a></li><li><a href="/blog-other2/超级访谈对话汤峥嵘">超级访谈对话汤峥嵘</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-other2/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>大数据<ul><li><a href="/blog-other2/从0开始学大数据">从0开始学大数据</a></li></ul></li><li>架构师<ul><li><a href="/blog-other2/sre实战手册">sre实战手册</a></li><li><a href="/blog-other2/vim实用技巧必知必会">vim实用技巧必知必会</a></li><li><a href="/blog-other2/深入浅出云计算">深入浅出云计算</a></li><li><a href="/blog-other2/说透数字化转型">说透数字化转型</a></li><li><a href="/blog-other2/遗留系统现代化实战">遗留系统现代化实战</a></li></ul></li><li>工作生活<ul><li><a aria-current="page" class="active" href="/blog-other2/机器学习40讲">机器学习40讲</a></li><li><a href="/blog-other2/职场求生攻略">职场求生攻略</a></li></ul></li><li>杂谈<ul><li><a href="/blog-other2/aI技术内参">aI技术内参</a></li><li><a href="/blog-other2/人工智能基础课">人工智能基础课</a></li><li><a href="/blog-other2/反爬虫兵法演绎20讲">反爬虫兵法演绎20讲</a></li><li><a href="/blog-other2/实用密码学">实用密码学</a></li><li><a href="/blog-other2/手把手带你写一门编程语言">手把手带你写一门编程语言</a></li><li><a href="/blog-other2/深入浅出区块链">深入浅出区块链</a></li><li><a href="/blog-other2/说透区块链">说透区块链</a></li><li><a href="/blog-other2/超级访谈对话汤峥嵘">超级访谈对话汤峥嵘</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-other2/机器学习40讲">机器学习40讲</a></li><li><a href="/blog-other2/机器学习40讲/01.开篇词">01.开篇词</a><ul><li><a href="/blog-other2/机器学习40讲/01.开篇词/01"><span>开篇词 | 打通修炼机器学习的任督二脉</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观">02.机器学习概观</a><ul><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/01"><span>01 | 频率视角下的机器学习</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/02"><span>02 | 贝叶斯视角下的机器学习</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/03"><span>03 | 学什么与怎么学</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/04"><span>04 | 计算学习理论</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/05"><span>05 | 模型的分类方式</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/06"><span>06 | 模型的设计准则</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/07"><span>07 | 模型的验证方法</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/08"><span>08 | 模型的评估指标</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/09"><span>09 | 实验设计</span></a></li><li><a href="/blog-other2/机器学习40讲/02.机器学习概观/10"><span>10 | 特征预处理</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-other2/机器学习40讲/03.统计机器学习模型">03.统计机器学习模型</a><ul><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/01"><span>11 | 基础线性回归：一元与多元</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/02"><span>12 | 正则化处理：收缩方法与边际化</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/03"><span>13 | 线性降维：主成分的使用</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/04"><span>14 | 非线性降维：流形学习</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/05"><span>15 | 从回归到分类：联系函数与降维</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/06"><span>16 | 建模非正态分布：广义线性模型</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/07"><span>17 | 几何角度看分类：支持向量机</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/08"><span>18 | 从全局到局部：核技巧</span></a></li><li><a aria-current="page" class="active" href="/blog-other2/机器学习40讲/03.统计机器学习模型/09"><span>19 | 非参数化的局部模型：K近邻</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/10"><span>20 | 基于距离的学习：聚类与度量学习</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/11"><span>21 | 基函数扩展：属性的非线性化</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/12"><span>22 | 自适应的基函数：神经网络</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/13"><span>23 | 层次化的神经网络：深度学习</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/14"><span>24 | 深度编解码：表示学习</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/15"><span>25 | 基于特征的区域划分：树模型</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/16"><span>26 | 集成化处理：Boosting与Bagging</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/17"><span>27 | 万能模型：梯度提升与随机森林</span></a></li><li><a href="/blog-other2/机器学习40讲/03.统计机器学习模型/18"><span>总结课 | 机器学习的模型体系</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型">04.概率图模型</a><ul><li><a href="/blog-other2/机器学习40讲/04.概率图模型/01"><span>28 | 最简单的概率图：朴素贝叶斯</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/02"><span>29 | 有向图模型：贝叶斯网络</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/03"><span>30 | 无向图模型：马尔可夫随机场</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/04"><span>31 | 建模连续分布：高斯网络</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/05"><span>32 | 从有限到无限：高斯过程</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/06"><span>33 | 序列化建模：隐马尔可夫模型</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/07"><span>34 | 连续序列化模型：线性动态系统</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/08"><span>35 | 精确推断：变量消除及其拓展</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/09"><span>36 | 确定近似推断：变分贝叶斯</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/10"><span>37 | 随机近似推断：MCMC</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/11"><span>38 | 完备数据下的参数学习：有向图与无向图</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/12"><span>39 | 隐变量下的参数学习：EM方法与混合模型</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/13"><span>40 | 结构学习：基于约束与基于评分</span></a></li><li><a href="/blog-other2/机器学习40讲/04.概率图模型/14"><span>总结课 | 贝叶斯学习的模型体系</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/05.结束语">05.结束语</a><ul><li><a href="/blog-other2/机器学习40讲/05.结束语/01"><span>结课 | 终有一天，你将为今天的付出骄傲</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/06.加餐">06.加餐</a><ul><li><a href="/blog-other2/机器学习40讲/06.加餐/01"><span>如何成为机器学习工程师？</span></a></li><li><a href="/blog-other2/机器学习40讲/06.加餐/02"><span>结课测试 | 这些机器学习知识你都掌握了吗？</span></a></li></ul></li><li><a href="/blog-other2/机器学习40讲/summary">机器学习40讲</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="19--非参数化的局部模型k近邻"><a aria-hidden="true" tabindex="-1" href="/blog-other2/机器学习40讲/03.统计机器学习模型/09#19--非参数化的局部模型k近邻"><span class="icon icon-link"></span></a>19 | 非参数化的局部模型：K近邻</h1><p>到目前为止，专栏中介绍的机器学习模型都属于参数模型，它们利用训练数据求解出关于问题的一般性知识，再将这些知识通过全局性模型的结构和参数加以外化。</p><p>一旦模型的结构和参数被确定，它们就不再依赖训练数据，可以直接用于未知数据的预测。而径向基核的出现一定程度上打破了这种规律，它将普适的全局特性打散成若干局部特性的组合，每个局部特性只能在它所覆盖的近邻区域内得以保持，由此产生的非结构化模型会具有更加灵活的表示能力。</p><p>在我看来，<strong>局部化的核心作用是模型复杂度和拟合精确性的折中</strong>。如果将整个输入空间看作一个大的整体区间，对它进行全局式的建模，那么单个模型就足以描述输入输出之间的规律，但这不可避免地会对表达能力造成较大的限制。</p><p>一个极端的情形是让所有输入的输出都等于同一个常数，这样的模型显然毫无信息量可言。可是在另一个极端，如果将局部特性继续加以细化，细化到让每个数据点都定义出不同局部特性的子区间，其结果就是基于实例的学习。</p><p><strong>基于实例的学习</strong>（instance-based learning）也叫<strong>基于记忆的学习</strong>（memory-based learning），<strong>它学习的不是明确的泛化模型，而是样本之间的关系</strong>。</p><p>当新的样本到来时，这种学习方式不会用拟合好的算式去计算输出结果或是输出结果的概率，而是根据这个新样本和训练样本之间的关系来确定它的输出。在本地化的语境里，这就叫“近朱者赤，近墨者黑”。</p><p>在基于实例的学习方法中，最典型的代表就是$k$近邻。$k$近邻算法（$k$-nearest neighbors algorithm）可能是最简单的机器学习算法，它将每个训练实例都表示成高维特征空间中的一个点。</p><p>在解决分类问题时，$k$近邻算法先找到高维空间中与未知实例最接近的$k$个训练实例，再根据少数服从多数的原则，将这$k$个实例中出现最多的类别标签分配给未知的实例。从贝叶斯定理的角度看，按照少数服从多数分配标签与后验概率最大化是等效的。</p><p>下图是$k$近邻算法的一个简单的例子。训练数据属于两个不同的类别，分别用蓝色方框和红色三角表示，绿色圆圈则代表待分类的数据点，其类别由$k$近邻算法决定。可以看到，当$k$等于3时，离未知数据最近的三个点是两红一蓝，因此数据会被归类为红色三角。可是当$k$从3增加到5时，多出来的两个实例都是蓝色的，这无疑会导致分类结果发生逆转。</p><p><img src="data:image/png;base64,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" alt=""/></p><p>k近邻算法示意图（图片来自维基百科）</p><p>这个例子说明了$k$近邻算法的一个特点，就是超参数$k$对性能的影响。作为一种局部加权模型，$k$近邻并不形成关于数据生成机制的全局性假设，而是刻画了数据在不同局部结构上的规律，局部结构的范围就是由$k$来定义的。</p><p>在本质上，超参数$k$和径向基核（以及其他的核函数）是一样的，只不过径向基核在定义局部结构时使用了连续分布的权值，所有数据对局部特征都有或大或小的贡献；而$k$近邻使用了离散分布的权值，只有部分足够接近的数据才有资格定义局部特征，它可以视为是可变带宽的径向基核。</p><p>从另一个角度看，超参数$k$表示了模型的复杂度，准确地说是和模型的复杂度成反比关系。如果训练集的容量为$N$，算法的有效参数数目就可以近似表示为$N / k$。</p><p>$k$均值的分类结果实质上是近邻区域内（就是上图中的圆圈）多个训练实例的平均，越大的$k$值意味着近邻区域包含的点数越多，平均化的程度就越高，对训练实例中噪声的平滑效果也就越好，相应的模型复杂度就越低。$k$的一个极端取值是直接等于训练集的容量，这相当于所有数据共同定义了同一个局部结构，这时的$k$近邻就退化成稳定的全局模型了。</p><p>反过来，越小的$k$值意味着近邻区域越狭窄，平均化的程度也就越低。这时的分类结果只由离未知数据点最近的少量训练实例决定，因而更容易受到这些实例中噪声的影响，也会表现出更强的过拟合倾向。当$k$等于1时，未知数据的类别只取决于离它最近的训练实例。</p><p>这时画出每个训练实例的近邻边界，所有的近邻边界共同构成了对特征空间的<strong>Voronoi划分</strong>（Voronoi tessellation）。当训练实例较多时，这种1近邻算法计算出的分类边界会非常复杂，其泛化性能较差。</p><p>除了超参数$k$之外，$k$近邻算法的另一个变量是对距离的定义方式，也就是如何衡量哪些点才是“近邻”的标准。最常用的距离度量无疑是<strong>欧氏距离</strong>，可除此之外，<strong>闵可夫斯基距离</strong>（Minkowski distance）、<strong>曼哈顿距离</strong>（Manhattan distance）和<strong>马氏距离</strong>（Mahalanobis distance）也可以应用在$k$近邻算法中，不同的距离代表的是对相似性的不同理解，在不同意义的相似性下，分类结果往往也会有所区别。这些距离是如何来定义的，你可以自己查阅。</p><p>对距离的依赖性给$k$近邻算法带来了一个新问题，那就是<strong>维数灾难</strong>。在之前介绍维数灾难时我曾经留了一个扣，现在就该解开它了。简而言之，维数灾难对$k$近邻算法的影响在于在高维空间中，曾经的近邻没有那么“近”了。不管特征空间的维度是多少，近邻区域的维度和特征空间的维度都是一致的。在这个前提下，要在特征维度增加时维持对特征空间的覆盖率不变，近邻区域在每个维度上的尺度就会越来越大。</p><p>维数灾难的几何意义其实可以直观地想象出来。如果特征空间是一条长度为1的一维直线，那任意一个长度为0.1的线段都能覆盖特征空间上10%的区域。可一旦特征空间变成二维，要在边长为1的正方形里圈出一个面积为0.1的小正方形，小正方形的边长就增加到$\sqrt<!-- -->{<!-- -->0.1<!-- -->}<!-- --> = 0.316$。这样一来，当数据点的数目不变时，维度的升高会导致原始的低维近邻点变得越来越稀疏，由近邻点所定义的局部结构也会越来越大。这样的局部结构失去了局部的意义，想让算法保证精确的分类性能就越困难了。</p><p><img src="/blog-other2/static/httpsstatic001geekbangorgresourceimage6c846c46d7f840c8e8badd37405d924fdc84.2fea520c.png" alt=""/></p><p>维度的升高会导致原始的低维近邻点变得越来越稀疏</p><p>（图片来自<a target="_blank" rel="noopener noreferrer" href="http://cleverowl.uk/2016/02/06/curse-of-dimensionality-explained/%EF%BC%89">http://cleverowl.uk/2016/02/06/curse-of-dimensionality-explained/）<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p><p>解决维数灾难最直接有效的方式就是<strong>增加数据点的数量</strong>，当数据点的数量没法增加时就得退而求其次，想办法<strong>降低特征空间的维度</strong>。还记得降维的主要方法吗？特征选择和特征提取都可以用于$k$近邻算法对数据的预处理。</p><p>作为典型的非参数方法，$k$近邻算法的运行方式和以线性回归为代表的参数方法截然相反。线性回归的运算量主要花在参数拟合上，利用大量的训练数据来拟合出使均方误差最小的一组参数。</p><p>一旦这组参数被计算出来，训练数据的历史使命就完成了，新来的数据都会用这组参数来处理。可$k$近邻算法的训练过程没那么复杂，只需要将数据存储下来就行了。可是对新实例进行分类时，$k$近邻算法需要找到离它最近的$k$个训练实例，这才是算法主要的运算负荷。</p><p>虽然是频率主义的方法，但以核函数和$k$近邻为代表的非参数方法也可以用来完成贝叶斯统计中<strong>概率密度估计</strong>（density estimation）的任务。如果用<strong>参数方法</strong>来进行密度估计，就需要先确定待估计概率密度的形式，再根据训练数据计算表示数字特征的参数。</p><p>比如假定概率密度具有正态分布的形式，那就需要估计它的均值和方差；具有指数分布的形式就要估计指数分布的参数。显然，参数化密度估计对概率密度形式的假设具有很强的依赖性，如果对概率分布的形式判断错误，那系数计算得再精确也是南辕北辙。</p><p>相比之下，非参数的密度估计就不会对待估计的分布做出什么先验假设，只是假定所有数据满足独立同分布的条件，因而具有更高的灵活性。但要讨论非参数密度估计方法，还是得从一种参数方法——<strong>直方图法</strong>（histogram）说起。</p><p>在统计学生成绩时，通常会计算&lt;60、60~70、70~80、80~90和&gt;90这些分数段内各有多少人，来大致绘出成绩的分布情况，这就是典型的直方图。直方图法将样本的取值范围划分成若干个等间隔子区间，再统计出现在每个子区间上的样本数目。在直方图上，第$i$个子区间上的概率可以表示成</p><p>$$ p_i = \dfrac<!-- -->{<!-- -->n_i<!-- -->}<!-- -->{<!-- -->N\Delta<!-- -->}<!-- --> $$</p><p>其中$n_i$是落在这个子区间内的样本数，$N$是样本容量，$\Delta$是每个子区间的宽度，它决定了直方图的分辨率。$\Delta$的取值过小会让直方图过于细密，让过多的局部细节掩盖了分布的整体结构；取值过大又会让直方图过于平滑，体现不出潜在的多模式趋势。要对概率密度做出精确的估计，必须要选择合适的区间宽度。</p><p><strong>直方图方法的一个问题在于计算出的概率密度不是连续函数</strong>。要解决这个问题，可以将原始的子区间替换成平滑的连续函数，这就让整体概率密度等于所有局部概率密度的叠加，避免了不连续点的出现。</p><p>那么用来插值的连续函数应该满足什么样的条件呢？平滑特性决定了它不能只管自己，也要刻画数据的局部特性。那么刻画局部特性的工具是什么呢？核函数嘛！把核函数用于密度估计，就是非参数的<strong>核密度估计方法</strong>（kernel density estimation）。</p><p><img src="/blog-other2/static/httpsstatic001geekbangorgresourceimage93a49324a3f05f928041202d39dc4624cfa4.f627d453.png" alt=""/></p><p>直方图（左）与核密度估计（右）（图片来自维基百科）</p><p>可有了核函数还不够，还需要确定它的带宽。在密度估计中，核函数带宽决定了局部结构的范围，其作用和直方图的子区间宽度类似。核函数的带宽过大会导致过度平滑，带宽过小则会导致欠平滑，在实际应用中可以通过最优化类似均方误差的指标来确定。</p><p>核密度估计虽然能够给出连续的概率密度，但它所有的局部结构都由相同的带宽决定。可是在特征空间上，不同区域数据的密度不同，其局部结构也应该有所区别，这时就需要引入$k$近邻算法的思想。在基于近邻的密度估计中，近邻点的数目$k$是唯一的参数，每个数据点的带宽就是第$k$个最近点和它的距离。和核密度估计的带宽一样，$k$值同样定义了局部结构的性质，因此在选择时也要慎之又慎。</p><p>核密度估计和近邻密度估计可以从一个统一的宏观视角加以审视。在高维空间中，如果将数据$\bf x$的局部结构定义为$R$，那么其概率密度就可以表示为</p><p>$$ p(<!-- -->{<!-- -->\bf x<!-- -->}<!-- -->) = \dfrac<!-- -->{<!-- -->K<!-- -->}<!-- -->{<!-- -->NV<!-- -->}<!-- --> $$</p><p>其中$K$表示$R$中的数据点数目，$V$表示$R$的体积，它们都是不确定的量。如果设置$V$固定、$K$可变来估计概率密度，得到的就是核密度估计；如果设置$K$固定、$V$可变来估计概率密度，得到的就是近邻密度估计。</p><p>当样本容量$N \rightarrow +\infty$时，$V$会随着$N$的增加而缩小，以保证$R$上的概率密度为常数；$K$则会随着$N$的增加而增加，以保证$R$上的概率密度存在明显的峰值。这时，两种非参数的估计结果都会收敛到真实的概率密度。</p><p>和支持向量机一样，$k$近邻算法也可以用来解决分类问题。利用Scikit-learn中的KNeighborsClassifier类，可以计算出曼城-西布朗数据集中的分类边界，其中$k$的取值分别被设置为1，7和15。可以看到，$k = 1$时所有训练数据都能正确分类，而$k = 15$时误分类率超过了10%。</p><p>这说明当超参数$k$的取值逐渐变大时，训练数据的误分类率在不断提升，但计算出的分类边界也变得越来越平滑。这是偏差-方差折中的典型体现。</p><p><img src="/blog-other2/static/httpsstatic001geekbangorgresourceimage1917198e43d514ea7ae95d20ef0a249cd717.5cb5026e.png" alt=""/></p><p>今天我和你分享了基于实例的学习方法，以及它的典型代表$k$近邻算法，其要点如下：</p><ul><li><p>基于实例的学习方法学的不是明确的泛化模型，而是样本之间的关系；</p></li><li><p>$k$近邻算法是非参数的局部化模型，具有无需训练的优点，但分类新实例的计算复杂度较高；</p></li><li><p>$k$近邻算法的性能取决于超参数$k$的取值和距离的定义方式；</p></li><li><p>核方法和近邻算法都可以用于数据的概率密度估计。</p></li></ul><p>$k$近邻方法是一种消极学习（lazy learning）方法，它并不会从训练数据中直接获取泛化决策，而是将它延迟到新样本出现的时候。相比之下，前面介绍的其他方法都属于积极学习（active learning）方法，在新样本出现前就做好了泛化工作。</p><p>那么，你觉得消极方法和积极方法有什么原理和性能上的优缺点呢？</p><p>欢迎分享你的观点。</p><p><img src="/blog-other2/static/httpsstatic001geekbangorgresourceimagec7d6c70b1d8ad0befe6c23c2d8ffc9b2f9d6.d56af573.jpg" alt=""/></p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/机器学习40讲/03.统计机器学习模型/09.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/29 16:53:57</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-other2/umi.2006439a.js"></script>
  </body>
</html>
